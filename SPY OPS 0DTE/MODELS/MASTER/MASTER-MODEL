# GRUMODEL

#region imports
from AlgorithmImports import *
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#endregion

class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.action = nn.Linear(output_size, 3)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        out, _ = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        action_scores = self.action(out)

        # Get the action with the highest score using argmax
        actions = torch.argmax(action_scores, dim=1)
        return actions


# A_Functions

#region imports
import gc
import os
import sys
import QuantConnect
cwd = os.getcwd()
functions_path_two = os.path.abspath(os.path.join(cwd, 'Research_Journals'))
sys.path.append(functions_path_two)
import B_Trend_Analyzer
from AlgorithmImports import *
from QuantConnect.Data.Market import TradeBar
from QuantConnect.Indicators import HullMovingAverage, ArnaudLegouxMovingAverage, SimpleMovingAverage
import math
import GRUModel
import pandas as pd
import numpy as np
import pickle
import time
import re

#endregion

contracts_dict = {}
daily_running_pnl = {}
starting_balance = 3000

def initialize_indicators(dictionary):
    print(f'ver. 5/5/23 - 2117 | initialize_indicators({dictionary["timescale"]})')
    moving_average_periods = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181]
    
    # Create new instances of the indicators
    Hull = HullMovingAverage(9)
    ALMA = ArnaudLegouxMovingAverage(9)
    SMAs = {period: SimpleMovingAverage(period) for period in moving_average_periods}
    TDSMAs = {period: SimpleMovingAverage(period) for period in moving_average_periods}
    TotalDif = 0

    # Update the dictionary with the new instances
    dictionary['HACD']['Hull'] = Hull
    dictionary['HACD']['ALMA'] = ALMA
    dictionary['HACD']['SMAs'] = SMAs
    dictionary['HACD']['TDSMAs'] = TDSMAs
    dictionary['HACD']['TotalDif'] = TotalDif

def extract_target_from_key(key):
    # Assuming that the target column always appears between the 2nd and 4th underscores (_), inclusive of the 3rd
    target_col = "_".join(key.split("_")[2:4])
    return target_col


def execute_action(action, contract_name, price, quantity, timestamp):
    global current_contracts
    global starting_balance
    fee = 3

    if action == 0:  # Hold
        reward = 0
    if action == 1:  # Buy
        if starting_balance >= price * quantity:
            contract_id = f"{contract_name}_{timestamp}"
            current_contracts[contract_id] = {'price': price, 'quantity': quantity}
            starting_balance -= price * quantity  # Deduct the price from the balance
            reward = (-price * quantity) - fee
        else:
            reward = 0
    elif action == 2:  # Sell
        buy_price = contract_info['average_cost']
        sell_price = contract_info['sell_price']
        reward = sell_price - buy_price - fee

    return reward

def update_contracts_dict(contract_name, action, price, quantity, timestamp):
    if contract_name not in contracts_dict:
        contracts_dict[contract_name] = {'positions': [], 'closed_positions': []}

    if action == 1:  # Buy
        # Find an existing open position or create a new one
        found_position = False
        for position in contracts_dict[contract_name]['positions']:
            if position['sell_quantity'] == 0:
                old_cost = position['average_cost'] * position['buy_quantity']
                new_cost = price * quantity
                position['buy_quantity'] += quantity
                position['average_cost'] = (old_cost + new_cost) / position['buy_quantity']
                found_position = True
                break

        if not found_position:
            contracts_dict[contract_name]['positions'].append({
                'average_cost': price,
                'buy_quantity': quantity,
                'sell_quantity': 0,
                'timestamp': timestamp
            })

    elif action == -1:  # Sell
        # Find the first open position and update it
        for position in contracts_dict[contract_name]['positions']:
            if position['sell_quantity'] < position['buy_quantity']:
                position['sell_quantity'] += quantity
                if position['sell_quantity'] == position['buy_quantity']:
                    contracts_dict[contract_name]['closed_positions'].append(position.copy())
                    position['sell_quantity'] = 0
                    position['average_cost'] = 0
                break

def clear_contracts_dict():
    contracts_dict.clear()

def timer(start_time):
    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)
    print(f"Elapsed time| {minutes}:{seconds}")

def HACDP(dictionary, price, time):
    
    def TotalDif(previous, hacd):
        if np.isnan(previous):
            if np.isnan(hacd):
                return 0
            else:
                return hacd
        elif np.isnan(hacd):
            return previous
        else:
            return previous + hacd

    timescale = dictionary['timescale']
    fast_ma = dictionary['HACD']['Hull']
    slow_ma = dictionary['HACD']['ALMA']

    if fast_ma.IsReady and slow_ma.IsReady:
        
        fast_ma = fast_ma.Current.Value
        slow_ma = slow_ma.Current.Value
        hacdp = ((fast_ma - slow_ma) / price)

        # Debugging NaN values
        if np.isnan(hacdp):
            print(f"NaN value found for HACD at time: {time}, fast_ma: {fast_ma}, slow_ma: {slow_ma}, price: {price}")

        for period in dictionary['HACD']['SMAs']:
            dictionary['HACD']['SMAs'][period].Update(time, hacdp)

        previous = dictionary['HACD']['TotalDif']
        dictionary['HACD']['TotalDif'] = TotalDif(previous, hacdp)

        for period in dictionary['HACD']['TDSMAs']:
            dictionary['HACD']['TDSMAs'][period].Update(time, dictionary['HACD']['TotalDif'])

        # Create a dictionary to store the HACD, SMAs, and TDSMAs values
        hacd_data = {'HACD': hacdp}
        # Accessing and updating SMAs
        for period, SMA in dictionary['HACD']['SMAs'].items():
            if SMA.IsReady:
                hacd_data[f'SMA_{period}'] = SMA.Current.Value
            else:
                hacd_data[f'SMA_{period}'] = -1e9

        # Access and update TDSMAs
        for period, TDSMA in dictionary['HACD']['TDSMAs'].items():
            if TDSMA.IsReady:
                hacd_data[f'TDSMA_{period}'] = TDSMA.Current.Value
            else:
                hacd_data[f'TDSMA_{period}'] = -1e9

        # Check for NaN values in hacd_data
        nan_keys = [key for key, value in hacd_data.items() if np.isnan(value)]
        if nan_keys:
            print(f"NaN value found in hacd_data at time: {time} for key(s): {nan_keys}")

        return time, hacd_data

    else:
        hacdp = -1e9
        hacd_data = {'HACD': hacdp}
        # Accessing and updating SMAs
        for period, SMA in dictionary['HACD']['SMAs'].items():
            hacd_data[f'SMA_{period}'] = -1e9
        # Access and update TDSMAs
        for period, TDSMA in dictionary['HACD']['TDSMAs'].items():
            hacd_data[f'TDSMA_{period}'] = -1e9

    return time, hacd_data

def checkNan(df, timescale):
    count = 0
    nan_df = df.isna()
    nan_columns = nan_df.any()
    nan_rows = nan_df.any(axis=1)
    if nan_rows.any():
        count += 1
        for column in df.columns:
            if nan_columns[column].any():
                print(f"  Column '{column}' contains NaN values: {df[nan_rows]}")
    if count == 0:
        print('No NaN values found in any DataFrame') 
        return True
    else:
        return False

def get_historical_options(start_date, end_date, qb, res):
    print(f'get_historical_options({start_date}, {end_date}, qb, {res})')
    # Fetch historical data for associated 0DTE options
    option_chain_provider = qb.OptionChainProvider
    option_symbols = option_chain_provider.GetOptionContractList(qb.AddEquity("SPY").Symbol, start_date)
    options_data = qb.History(option_symbols, start_date, end_date, res)

    # Filter 0DTE options
    options_data = options_data.loc[(options_data["expiry"] - options_data.index.get_level_values("time")).days == 0]

    # Reset index and return data
    options_data.reset_index(inplace=True)

    # Add 'options_' prefix to each column name
    options_data = options_data.add_prefix('options_')

    return options_data


def update_indicators(dictionary, history, symbol, timescale):
    print(f'update_indicators(dictionary, history, {symbol}, {timescale}')
    def process_history(dictionary, timescale, history, timedelta_value):
        print(f'process_history(dictionary, {timescale}, history, {symbol}, {timedelta_value}')
        trade_bars = []
        for time, bar in history.iterrows():
            time = time - timedelta_value
            tradeBar = TradeBar(time, symbol, bar.open, bar.high, bar.low, bar.close, bar.volume, timedelta_value)
            trade_bars.append(tradeBar) 

        trade_bars_dict = []
        for trade_bar in trade_bars:
            trade_bar_dict = {
                'time': trade_bar.Time,
                'symbol': trade_bar.Symbol,
                f'{timescale}_open': trade_bar.Open,
                f'{timescale}_high': trade_bar.High,
                f'{timescale}_low': trade_bar.Low,
                f'{timescale}_close': trade_bar.Close,
                f'{timescale}_volume': trade_bar.Volume,
                f'{timescale}_period': trade_bar.Period
            }
            trade_bars_dict.append(trade_bar_dict)

        column_names = ['time'] + [k for k in trade_bars_dict[0].keys() if k not in ['time', 'symbol']]
        df = pd.DataFrame(trade_bars_dict, columns=column_names)
        df.set_index('time', inplace=True)

        # Initialize empty lists to store datetime and HACD values
        datetime_list = []
        hacd_data_list = []

        # Iterate over the rows in the history DataFrame
        for time, row in df.iterrows():
            price = row[f'{timescale}_close']
        
            fast_ma = dictionary['HACD']['Hull']
            slow_ma = dictionary['HACD']['ALMA']

            fast_ma.Update(time, price)
            slow_ma.Update(time, price)

            # Calculate HACD values using the HACDP() method
            datetime, hacd_data = HACDP(dictionary, price, time)
            if datetime is not None:
                datetime_list.append(datetime)
                hacd_data_list.append(hacd_data)

        # Create a new DataFrame with datetime as the index and HACD, SMAs, and TDSMAs values
        moving_average_periods = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181]
        column_names = ['HACD'] + [f'SMA_{period}' for period in moving_average_periods] + [f'TDSMA_{period}' for period in moving_average_periods]
        hacd_df = pd.DataFrame(hacd_data_list, columns=column_names)
        hacd_df.index = pd.to_datetime(datetime_list)


        # Concatenate the history DataFrame with the HACD DataFrame
        combined_df = pd.concat([df, hacd_df], axis=1)

        return combined_df

    # Process the history using the process_history function from MLGetHistory
    timedelta = dictionary['timedelta']
    processed_history = process_history(dictionary, timescale, history, timedelta)

    return processed_history


def checkNanTB(trade_bars, trade_bar_dicts):
    # debugging NaN
    # Check if the list contains any NaN values
    nan_trade_bars = []
    for trade_bar in trade_bars:
        nan_values = {
            'open': math.isnan(trade_bar.Open),
            'high': math.isnan(trade_bar.High),
            'low': math.isnan(trade_bar.Low),
            'close': math.isnan(trade_bar.Close),
            'volume': math.isnan(trade_bar.Volume)
        }
        nan_attributes = [attr for attr, is_nan in nan_values.items() if is_nan]
        if nan_attributes:
            nan_trade_bars.append({'time': trade_bar.Time, 'nan_attributes': nan_attributes})
    print("TradeBars with NaN values:")
    count = 0
    for nan_trade_bar in nan_trade_bars:
        print(f"{nan_trade_bar['time']}: {nan_trade_bar['nan_attributes']}")
        count += 1
    if count == 0:
        print('NONE')
    # Check if the dictionaries contain any NaN values
    nan_trade_bar_dicts = []
    for trade_bar_dict in trade_bar_dicts:
        nan_values = {
            key: math.isnan(value) for key, value in trade_bar_dict.items() if key not in ['time', 'symbol', 'period']
        }
        nan_attributes = [attr for attr, is_nan in nan_values.items() if is_nan]
        if nan_attributes:
            nan_trade_bar_dicts.append({'time': trade_bar_dict['time'], 'nan_attributes': nan_attributes})
    print("Dictionaries with NaN values:")
    count2 = 0
    for nan_trade_bar_dict in nan_trade_bar_dicts:
        print(f"{nan_trade_bar_dict['time']}: {nan_trade_bar_dict['nan_attributes']}")
        count2 += 1
    if count2 == 0:
        print('NONE')

def checkIndices(df_list, name):
    base_index = df_list[0].index
    matching_indices = [True] * len(df_list)
    
    for i, df in enumerate(df_list[1:], start=1):
        if not base_index.equals(df.index):
            matching_indices[i] = False

    if all(matching_indices):
        print(f"All {name} indices match.")
    else:
        print("Indices of the DataFrames do not match.")
        for i, match in enumerate(matching_indices):
            if not match:
                print(f"{name} {i} index does not match base index.")
            else:
                print(f"{name} {i} index matches base index.")


def prepare_concatenated_dataframes(stored_dictionaries, updated_indicators, target_columns):
    print(f'Updated: prepare_concatenated_dataframes(stored_dictionaries, updated_indicators, {target_columns})')
    df_concat_list = []
    final_df_list = []

    one_min_df = updated_indicators[0]

    for index, df in enumerate(updated_indicators):
        timescale = stored_dictionaries[index]['timescale']

        if index == 0:
            df_concat_list.append(df)
        else:
            df = df.resample('1min').ffill(limit=None)
            df = df.reindex(one_min_df.index)
            df.fillna(method='ffill', inplace=True)  # Replace interpolation with forward-fill
            df_concat_list.append(df)

    for df, target_col in zip(df_concat_list, target_columns):    
        # Merge the target columns
        target_df = [df for df in updated_indicators if target_col in df.columns][0]
        df = df.merge(target_df[[target_col]], left_index=True, right_index=True, suffixes=('', '_y'))
        columns = df.columns.tolist()
        if f'{target_col}_y' in columns:
            df.drop(columns=[f'{target_col}_y'], inplace=True)
            print(f'DUPLICATE TARGET COLUMN {target_col} REMOVED.')
        final_df_list.append(df)

    return final_df_list



# New function to create the main DataFrame
def create_main_dataframe(predictions_list, combined_error_metrics, options_data, spy_data):
    print(f'create_main_dataframe(predictions_list, combined_error_metrics, options_data, spy_data)')
    # Drop the 'symbol' level from the MultiIndex in options_data and spy_data
    options_data.index = options_data.index.droplevel('symbol')
    spy_data.index = spy_data.index.droplevel('symbol')

    # Concatenate the predictions and error metrics
    predictions_df = pd.concat(predictions_list, axis=1)
    main_df = pd.concat([predictions_df, combined_error_metrics, options_data, spy_data], axis=1)
    
    # Add an initial 'PnL' column with all values set to 0
    main_df['PnL'] = 0

    return main_df


def load_sub_models(qb):
    print(f'# load_sub_models()')
    model_list = ['14122444_HACD_V_High_models_postMseMaeError', '14122444_HACD_V_Low_models_postMseMaeError', '14122444_HACD_M_Low_models_postMseMaeError', '14122444_HACD_M_High_models_postMseMaeError']
    sub_models = []

    for key in model_list:
        if qb.ObjectStore.ContainsKey(key):
            deserialized = bytes(qb.ObjectStore.ReadBytes(key))
            model_data = (pickle.loads(deserialized))
            # print(f"Model data for key {key}: {model_data}")  # Print the model_data
            sub_models.append((key, model_data))  # Store the key and model_data as a tuple
        else:
            print(f'ERROR: {key} NOT IN OBJECT STORE')
        
    return sub_models


def generate_submodel_predictions(updated_indicators, sub_models_dict):
    # Initialize the results dictionary
    results = {}

    # Iterate through the sub_models_dict
    for target, sub_models in sub_models_dict.items():
        # Initialize the results dictionary for the current target
        results[target] = {}

        for sub_model_key, sub_model_data, in zip(sub_models.keys(), sub_models.values()):
            # Extract the required data from the sub_model_data dictionary
            model = sub_model_data['model']
            scaler = sub_model_data['scaler']

            # Get the updated_indicators DataFrame for the current timescale
            df = updated_indicators[sub_model_key]

            # Merge the target columns
            target_df = [df for df in updated_indicators.values() if target in df.columns][0]
            df = df.merge(target_df[[target]], left_index=True, right_index=True, suffixes=('', '_y'))
            columns = df.columns.tolist()
            if f'{target}_y' in columns:
                df.drop(columns=[f'{target}_y'], inplace=True)
                print(f'DUPLICATE TARGET COLUMN {target} REMOVED.')

            input_dataframe = df
            # Get the list of period columns
            period_columns = input_dataframe.filter(like='period').columns
            # Combine the target and period columns into a list
            columns_to_drop = [target] + period_columns.tolist()

            # scale 
            X = scaler.transform(df.drop(columns_to_drop, axis=1))

            # Generate the predictions using the sub-model
            predictions = model.predict(X)

            # Store the predictions in the results dictionary
            results[target][sub_model_key] = predictions

    return results



def initialize_master_model(main_df):
    print(f'initialize_master_model(main_df)')

    '''For the hidden size, number of layers, and output size, you can choose appropriate values 
    based on your specific problem and model complexity. It's common to start with a small to moderate 
    number of hidden units and layers, and then increase them if needed to improve model performance.'''

    input_size = main_df.shape[1]
    hidden_size = 64
    num_layers = 2
    output_size = 1  # Equal to number of target variables

    master_model = GRUModel(input_size, hidden_size, num_layers, output_size).to(device)
    
    return master_model

def calculate_error_metrics(sub_model_predictions, sub_models_list):
    print(f'calculate_error_metrics(sub_model_predictions, sub_models_list)')
    error_metrics_dfs = []
    
    for i, sub_model_data in enumerate(sub_models_list):
        sub_model_key = sub_model_data['key']  # Assuming you added the key to the sub_model_data dictionary
        print(f"Calculating error metrics for {sub_model_key}")

        # Extract the true values and datetime indices from the sub-model test set
        true_values = sub_model_data['y_test_inverse'].flatten()
        datetime_indices = sub_model_data['test_datetime_indices']

        # Calculate squared errors and absolute errors for each data point
        predictions = sub_model_predictions[i]
        squared_errors = [(true_values[j] - predictions[j])**2 for j in range(len(predictions))]
        absolute_errors = [abs(true_values[j] - predictions[j]) for j in range(len(predictions))]
        
        # Create a DataFrame to store the error metrics and datetime index
        error_metrics_df = pd.DataFrame({
            'datetime': datetime_indices,
            'squared_error': squared_errors,
            'absolute_error': absolute_errors
        })

        # Set the datetime index for the DataFrame
        error_metrics_df.set_index('datetime', inplace=True)
        
        # Add the error metrics DataFrame to the list
        error_metrics_dfs.append(error_metrics_df)
    
    return error_metrics_dfs


def calculate_reward(action, contract_info):
    fee = 3

    if action == 0:  # Hold
        reward = 0
    elif action == 1:  # Buy
        reward = -fee
    elif action == 2:  # Sell
        buy_price = contract_info['average_cost']
        sell_price = contract_info['sell_price']
        reward = sell_price - buy_price - fee

    return reward


def closest_divisor(n, divisor_value):
    print(f'closest_divosr({n}, {divisor_value})')
    result = set()
    for i in range(1, int(n**0.5) + 1):
        div, mod = divmod(n, i)
        if mod == 0:
            result |= {i, div}

    factors_list = sorted(list(result))
    closest_divisor = min(factors_list, key=lambda x: abs(x - divisor_value))

    return closest_divisor


def train_master_model(qb, main_df, master_model):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ProjectID = '14122444'
    print(f'TRAINING MASTER MODEL...')

    # Select the columns you want to use as features
    feature_columns = [col for col in main_df.columns if 'running_PnL' not in col]
    features_data = main_df[feature_columns].values.astype(np.float32)
    feature_count = len(feature_columns)
    print(f'{feature_count} FEATURES SELECTED...')
    print(feature_columns)

    # Normalize the data
    scaler = MinMaxScaler(feature_range=(-1, 1))
    features_data_normalized = scaler.fit_transform(features_data)

    # Set the train_ratio and seq_length variables
    train_ratio = 0.8
    seq_length = 60

    # Extract y_train (the target variable) from main_df
    y_train = main_df['running_PnL'].values.astype(np.float32)

    # Scale the target variable
    target_scaler = MinMaxScaler()
    y_train = target_scaler.fit_transform(y_train.reshape(-1, 1))

    # Create sequences and targets, where target_column is the index of the target variable (
    # e.g., the index of 'V_low' in feature_columns)
    target_column = feature_columns.index('running_PnL')

    X = []
    y = []

    num_train_sequences = int((len(features_data_normalized) - seq_length) * train_ratio)
    num_test_sequences = len(features_data_normalized) - seq_length - num_train_sequences

    for i in range(len(features_data_normalized) - seq_length - 1):
        X_seq = features_data_normalized[i:i + seq_length]
        y_target = features_data_normalized[i + seq_length, target_column]
        mask_seq = mask[i:i + seq_length]
        X.append(X_seq)
        y.append([[y_target]])

    X = np.stack(X)
    y = np.concatenate(y, axis=0)

    # Split the data into training and testing sets
    X_train = X[:num_train_sequences]
    y_train = y[:num_train_sequences]
    X_test = X[num_train_sequences:]
    y_test = y[num_train_sequences:]

    mask_train = mask_sequences[:num_train_sequences]
    mask_test = mask_sequences[num_train_sequences:]

    print(f'DATA SPLIT (MASK/TRAINING/TEST) COMPLETED...')

    print("X_train shape:", X_train.shape)
    print("y_train shape:", y_train.shape)
    print("mask_train shape:", mask_train.shape)
    print("X_test shape:", X_test.shape)
    print("y_test shape:", y_test.shape)
    print("mask_test shape:", mask_test.shape)


    # Replace nan values with 0
    X_train[np.isnan(X_train)] = 0
    X_test[np.isnan(X_test)] = 0

    # Create PyTorch datasets and dataloaders
    batch_size = 64

    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train), torch.from_numpy(mask_train))
    test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test), torch.from_numpy(mask_test))

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f'PYTORCH DATA SETS AND LOADERS CREATED...')

    # Define inputs and GRU model || HYPERPARAMETERS

    input_size = feature_count
    hidden_size = 64
    num_layers = 2
    output_size = 1

    model = GRUModel(input_size, hidden_size, num_layers, output_size).to(device)

    print(f'INPUTS AND GRU MODEL DEFINED...')

    # Define the loss function and optimizer 
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    num_epochs = 50
    steps_per_epoch = math.ceil(num_train_sequences / batch_size)
    save_after = 10
    closest_divisor = closest_divisor(steps_per_epoch, save_after)
    save_every_n_steps = steps_per_epoch/closest_divisor

    print(f'LOSS FUNCTION AND OPTIMIZER DEFINED...')

    # Load the latest saved model from QuantConnect's ObjectStore, if available
    start_epoch = 0
    start_step = 0
    model_key = f'{ProjectID}_saved_HACD_Master_model'
    print(f'SEARCHING FOR {model_key} SAVE FILE...')

    if qb.ObjectStore.ContainsKey(model_key):
        # Load the serialized data from QuantConnect's ObjectStore

        saved_model_bytes = qb.ObjectStore.ReadBytes(model_key)

        # Convert the .NET byte array to a Python io.BytesIO object
        saved_model_bytes_io = io.BytesIO(saved_model_bytes)

        # Deserialize the saved_model dictionary
        saved_model = pickle.load(saved_model_bytes_io)

        # Load the state dictionaries into the model and optimizer
        model.load_state_dict(saved_model['model_state_dict'])
        optimizer.load_state_dict(saved_model['optimizer_state_dict'])
        start_epoch = saved_model['epoch']
        start_step = saved_model['step']
        performance_metrics = saved_model['performance_metrics']
        test_losses = saved_model['performance_metrics']['test_losses']
        print(f'RESUMING TRAINING EPOCH {start_epoch}, STEP {start_step}...')
    else:
        print(f'/ NO {model_key} SAVE FILE /')
    
    # Train the model
    print(f'TRAINING MASTER MODEL...')
    test_losses = []
    for epoch in range(start_epoch, num_epochs):
        print(f'...EPOCH {epoch+1}/{num_epochs}')
        step = start_step
        test_loss = 0.0
        model.train()
        for batch_idx, (X_batch, y_batch, mask_batch) in enumerate(train_loader):
            # Get the model's buy/sell decisions for the current batch
            actions = model(X_batch.to(device), mask_batch.to(device))

            for i, action in enumerate(actions):
                contract_name = main_df['contract_name'].iloc[batch_idx * batch_size + i]
                price = main_df['price'].iloc[batch_idx * batch_size + i]
                timestamp = main_df['timestamp'].iloc[batch_idx * batch_size + i]
                quantity = main_df['quantity'].iloc[batch_idx * batch_size + i]

                # Update the contracts_dict based on the action taken
                contract_info = update_contracts_dict(contract_name, action.item(), price, quantity, timestamp)

                # Calculate the reward based on the action taken
                reward = execute_action(action.item(), contract_name, contract_info, contracts_dict)

                # Update the running_PnL column with the reward
                main_df.loc[batch_idx * batch_size + i, 'running_PnL'] += reward

                # Reset the running_PnL for a new day
                prev_day = main_df['timestamp'].iloc[batch_idx * batch_size + i - 1].date()
                current_day = timestamp.date()
                if current_day != prev_day:
                    daily_running_pnl[prev_day] = main_df.loc[batch_idx * batch_size + i - 1, 'running_PnL']
                    starting_balance += daily_running_pnl[prev_day]  # Update the balance with the running_PnL
                    main_df.loc[batch_idx * batch_size + i, 'running_PnL'] = 0
                    clear_contracts_dict()

                # Update the target variable (Q-value) based on the running_PnL
                y_batch = torch.tensor(main_df.loc[batch_idx * batch_size + i, 'running_PnL'], dtype=torch.float32).to(device)

            # Zero the gradients
            optimizer.zero_grad()

            # Calculate the loss
            loss = criterion(actions, y_batch)

            # Backpropagation
            loss.backward()

            # Update the weights
            optimizer.step()

            # Save the model after every n steps
            if step % save_every_n_steps == 0:
                saved_model = {
                    'epoch': epoch,
                    'step': step + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'performance_metrics': {'daily_running_pnl': daily_running_pnl}
                }
                save(qb, saved_model, f'saved_HACD_Master_model')

       # Evaluate the model on the test set
        model.eval()
        test_loss = 0
        predictions = []
        squared_errors = []
        absolute_errors = []

        with torch.no_grad():
            for batch_idx, (X_batch, y_batch, mask_batch) in enumerate(test_loader):
                X_batch, y_batch, mask_batch = X_batch.to(device), y_batch.to(device), mask_batch.to(device)
                actions = model(X_batch, mask_batch)
                test_loss += criterion(actions, y_batch).item() * X_batch.size(0)

                for i, action in enumerate(actions):
                    contract_name = main_df['contract_name'].iloc[batch_idx * batch_size + i]
                    price = main_df['price'].iloc[batch_idx * batch_size + i]
                    timestamp = main_df['timestamp'].iloc[batch_idx * batch_size + i]
                    quantity = main_df['quantity'].iloc[batch_idx * batch_size + i]

                    # Update the contracts_dict based on the action taken
                    contract_info = update_contracts_dict(contract_name, action.item(), price, quantity, timestamp)

                    # Calculate the reward based on the action taken
                    reward = execute_action(action.item(), contract_name, contract_info)

                    # Update the running_PnL column with the reward
                    main_df.loc[batch_idx * batch_size + i, 'running_PnL'] += reward

                    # Reset the running_PnL for a new day
                    prev_day = main_df['timestamp'].iloc[batch_idx * batch_size + i - 1].date()
                    current_day = timestamp.date()
                    if current_day != prev_day:
                        daily_running_pnl[prev_day] = main_df.loc[batch_idx * batch_size + i - 1, 'running_PnL']
                        starting_balance += daily_running_pnl[prev_day]
                        main_df.loc[batch_idx * batch_size + i, 'running_PnL'] = 0
                        clear_contracts_dict()

                    # Calculate squared errors and absolute errors for each data point
                    squared_error = (y_batch[i].item() - actions[i].item())**2
                    absolute_error = abs(y_batch[i].item() - actions[i].item())
                    squared_errors.append(squared_error)
                    absolute_errors.append(absolute_error)

        # Calculate the overall MSE and MAE
        mse = np.mean(squared_errors)
        mae = np.mean(absolute_errors)

        test_loss /= num_test_sequences
        test_losses.append(test_loss)

        # Invert the scaling of predictions and true values
        predictions = target_scaler.inverse_transform(np.array(predictions).reshape(-1, 1))
        y_test_inverse = target_scaler.inverse_transform(y_test.reshape(-1, 1))

        performance_metrics = {
            'test_losses': test_losses, 'predictions': predictions, 'y_test_inverse': y_test_inverse, 
            'mse': mse, 'mae': mae, 'daily_running_pnl': daily_running_pnl}

        saved_model = {
            'epoch': epoch + 1,
            'step':0,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'performance_metrics': performance_metrics
        }
        save = save(qb, saved_model, 'saved_HACD_Master_model')
        if qb.ObjectStore.ContainsKey(save): 
            print(f'{save} SAVED SUCCESSFULLY')
        else:
            print(f'ERROR {save} KEY NOT IN OBJECT STORE')

    print(f'TRAINING COMPLETE...')

    # Return the trained model and any relevant information (e.g., scaler, performance metrics)
    return {
        'model': model,
        'scaler': scaler,
        'performance_metrics': performance_metrics,
        'test_losses': test_losses
    }

    
def save(qb, file, name, project_id):
    serialized = pickle.dumps(file)
    
    key = f'{project_id}_{name}'
    
    save = qb.ObjectStore.SaveBytes(key, serialized)
    print(f'{qb.Time}  SAVED: {key}')
    return key


# A_Main

%load_ext autoreload
%reload_ext autoreload
%autoreload 2

import gc
import os
import sys
import QuantConnect
cwd = os.getcwd()
functions_path = os.path.abspath(os.path.join(cwd, 'Research_Journals', 'Master_Model'))
sys.path.append(functions_path)
import A_Functions
import B_Trend_Analyzer
import GRUModel
import pickle
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta


self = QuantBook()
print(f'indicators_key')
ID = 14122444

self.udl = self.AddEquity('SPY', Resolution.Minute, Market.USA, True, 1, True)
self.udl.SetDataNormalizationMode(DataNormalizationMode.Raw)
symbol = self.udl.Symbol

# Define the time range for the data
start_date_str = '2018-01-01'
end_date_str = '2020-12-31'

start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
end_date = datetime.strptime(end_date_str, '%Y-%m-%d')

# Initialize the required data/variables
bar_count = timedelta(days=100)
res = Resolution.Minute
spy_data = self.History(symbol, start_date, end_date, res).loc[symbol]

# initialize dictionaries and indicators
key = f'{ID}_DictionaryList'
updated_indicators = []
start_time = time.time()
if self.ObjectStore.ContainsKey(key):
    deserialized = bytes(self.ObjectStore.ReadBytes(key))
    storedDictionaries = (pickle.loads(deserialized)) 
    updated_indicators = {}

    for dictionary in storedDictionaries:
        timescale = dictionary['timescale']
        indicators_key = f'{timescale}_'
        name = f'{timescale}_HACD_master_initializer'
        save_key = f'{ID}_{name}'
        if self.ObjectStore.ContainsKey(save_key):
            deserialized = bytes(self.ObjectStore.ReadBytes(save_key))
            update = (pickle.loads(deserialized)) 
            check = True
        else:
            initialize = A_Functions.initialize_indicators(dictionary)
            print(f'NO SAVE FILE // UPDATING {timescale} INDICATORS')
            update = A_Functions.update_indicators(dictionary, spy_data, symbol, timescale)
            check = A_Functions.checkNan(update, timescale)
        if check:
            updated_indicators[indicators_key] = update
            save = A_Functions.save(self, update, name, ID)
            if self.ObjectStore.ContainsKey(save): 
                print(f'{save} SAVED SUCCESSFULLY')
            else:
                print(f'ERROR: {save} KEY NOT IN OBJECT STORE')
            A_Functions.timer(start_time)
        else:
            A_Functions.timer(start_time)
            print(f'ERROR: CHECK FAILED FOR {timescale}')
            self.Quit()
        
else:
    print(F'ERROR: {key}')

# Generate predictions and error metrics for the current data
sub_models_list = A_Functions.load_sub_models(self)
A_Functions.timer(start_time)
sub_models_dict = {A_Functions.extract_target_from_key(key).capitalize(): sub_model_data for key, sub_model_data in sub_models_list}
sub_model_predictions = A_Functions.generate_submodel_predictions(updated_indicators, sub_models_dict)
A_Functions.timer(start_time)

# Calculate the error metrics for the current data
A_Functions.timer(start_time)
error_metrics_dfs = A_Functions.calculate_error_metrics(sub_model_predictions, sub_models_list)
A_Functions.timer(start_time)
combined_error_metrics = pd.concat(error_metrics_dfs, axis=0)
check = A_Functions.checkIndices(combined_error_metrics, 'combined_error_metrics')

# Create the main DataFrame with sub-model predictions, error metrics, options, and underlying
A_Functions.timer(start_time)
options_data = A_Functions.get_historical_options(start_date, end_date, self, res)
A_Functions.timer(start_time)
main_df = A_Functions.create_main_dataframe(predictions_list, combined_error_metrics, options_data, spy_data)
A_Functions.timer(start_time)
check = A_Functions.checkIndices(main_df, 'main_df')

# Train the master model using the main DataFrame
master_model = A_Functions.initialize_master_model(main_df)
A_Functions.timer(start_time)
master_model_info = train_master_model(self, main_df, master_model)
A_Functions.timer(start_time)
master_model = master_model_info['model']

